import re
import json
from typing import Dict, Any, List
from mecab import MeCab
from datetime import datetime, timedelta
import os

class Parser:
    def __init__(self):
        self.tokenizer = MeCab()
        print("Parser ì´ˆê¸°í™” ì™„ë£Œ: Mecab ì—”ì§„ ì‚¬ìš©")

        self.special_words = [
            "ì—½ë–¡", "ì§œíŒŒêµ¬ë¦¬", "ë§ë‹´", "ì¸ê°•", "ì¿ íŒ¡", "ë°°ë¯¼", "ìš”ê¸°ìš”", "ë¡œì œ", "í˜¼ìˆ ",
            "í˜¼ë°¥", "ì†Œí™•í–‰", "í‡´ê·¼ê¸¸", "ì¶œê·¼ê¸¸", "ì ë©”ì¶”", "ì•„ì•„", "ì•„ë©”", "ì•„ì¹´",
            "ì•„ì¹´í˜ë¼", "ì¹´í˜ë¼ë–¼", "ì¹´í˜ëª¨ì¹´", "ì¹´ëª¨", "ì¹´ëª¨ì¹´", "í—¬ìŠ¤ì¥", "êµì´Œì¹˜í‚¨", 
            "í¬íŠ¸í´ë¦¬ì˜¤", "ì±„ìš©ê³µê³ ", "ê²½ì°°ì„œ" # ë³µí•© ëª…ì‚¬ ì¶”ê°€ ìœ ì§€
        ]

        # ë¬¸ì¥ ë¶„ë¦¬ ê¸°ì¤€ í’ˆì‚¬/í† í° ëª©ë¡
        self.SPLIT_TOKENS = {
            "EC",  # ì—°ê²° ì–´ë¯¸: -ê³ , -ìœ¼ë©° ë“±
            "MAJ",  # ì ‘ì† ë¶€ì‚¬: ê·¸ë¦¬ê³ , ê·¸ëŸ¬ë‚˜ ë“±
            "SF"   # ë§ˆì¹¨í‘œ
        }
        # í† í° í…ìŠ¤íŠ¸ ê¸°ì¤€ (ê°•ì œ ë¶„ë¦¬ ë° í•„í„°ë§)
        # ğŸš¨ FIX 2: 'ë˜', 'ê³ 'ë¥¼ ì‚­ì œí•˜ì—¬ ë¬¸ì¥ ë¶„ë¦¬ ê¸°ì¤€ì„ ì™„í™”í•˜ê³ , ë¶ˆí•„ìš”í•œ ê³¼ë¶„ë¦¬ ë°©ì§€
        self.SPLIT_TEXTS = [
            "ê·¸ë¦¬ê³ ", "ê·¸ëŸ¬ê³ ", "í•´ì•¼ì§€", "í•´ì•¼ê² ë‹¤", "í•´ì•¼ë¼", "í•´ì•¼ë§Œ", "í•˜ê³ ", "ì´ê³ ", 
            "ë˜ë‹ˆ", "ë˜ì„œ", "ë˜ê³ ", "ë¼" 
        ]

    # --- ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œ ì¶”ê°€ ---
    def _get_verb_root(self, token: str) -> str:
        """ ë™ì‚¬ í† í°ì—ì„œ ì–´ë¯¸ë¥¼ ì œê±°í•˜ê³  ì›í˜•ì„ ì¶”ì¶œí•˜ëŠ” íœ´ë¦¬ìŠ¤í‹± """
        
        # 'ë˜' ê³„ì—´ì€ ìµœì¢… ë™ì‚¬ ì›í˜•ìœ¼ë¡œ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
        if token in ["ë˜", "ë¼", "ë˜ê³ ", "ë˜ì„œ", "ë˜ì–´"]:
             return ""

        # 1. ì¼ë°˜ì ì¸ ì–´ë¯¸ ì œê±° íœ´ë¦¬ìŠ¤í‹± (ì¢…ê²°/ì—°ê²° ì–´ë¯¸)
        root = re.sub(r'(ì•„|ì–´|ì´|ì•¼|ì§€|ë‹¤|ê³ |ë„¤|ë‹ˆ|ì•¼ì§€|ì–´ì•¼ì§€|ì•„ì•¼ì§€|ã„¹ê¹Œ|ã„¹ê²Œ|ã…‚ë‹ˆë‹¤|ìŠµë‹ˆë‹¤|ìš”)$', '', token, flags=re.IGNORECASE)
        
        # 2. 'í•´' -> 'í•˜' ì²˜ë¦¬
        if root == "í•´":
            return "í•˜"
        
        # 3. í›„ì²˜ë¦¬: ì œê±° í›„ ë¹ˆ ë¬¸ìì—´ì´ ë˜ëŠ” ê²½ìš° (ì˜ˆ: 'ê°€' + 'ê³ ' -> 'ê°€')
        if not root and len(token) > 1 and token[-1] in ['ê³ ', 'ì„œ', 'ë‹ˆ', 'ì•¼']:
            return token[:-1]
            
        return root if root else ""

    def _split_sentences(self, text: str) -> List[str]:
        """ 
        ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ ë„ì–´ì“°ê¸°ë¥¼ ìµœëŒ€í•œ ë³´ì¡´í•˜ë©° ë¬¸ì¥ ë¶„ë¦¬ ê¸°ì¤€ì— ë”°ë¼ ë‚˜ëˆ•ë‹ˆë‹¤.
        """
        
        split_tokens_pattern = '|'.join(map(re.escape, self.SPLIT_TEXTS))
        split_pattern_regex = r'(' + split_tokens_pattern + r'|\s*[.,?!]\s*)'

        sentences_and_splitters = re.split(split_pattern_regex, text)
        
        cleaned_sentences = []
        current_sentence = ""

        for part in sentences_and_splitters:
            if part is None or not part.strip():
                continue
            
            is_splitter = any(part.strip() == t for t in self.SPLIT_TEXTS) or re.match(r'^\s*[.,?!]\s*$', part.strip())
            
            if is_splitter:
                if current_sentence.strip():
                    cleaned_sentences.append(current_sentence.strip())
                current_sentence = ""
            else:
                if current_sentence and not current_sentence.endswith(' ') and not part.startswith(' '):
                     current_sentence += ' ' 
                current_sentence += part

        if current_sentence.strip():
            cleaned_sentences.append(current_sentence.strip())
            
        return [s for s in cleaned_sentences if s]


    def _get_absolute_date(self, relative_date: str) -> str:
        """ ìƒëŒ€ì  ë‚ ì§œë¥¼ ì ˆëŒ€ ë‚ ì§œë¡œ ë³€í™˜ """
        today = datetime.today()
        if relative_date == "ì˜¤ëŠ˜" or relative_date == "":
            return today.strftime("%Y-%m-%d")
        elif relative_date == "ë‚´ì¼":
            tomorrow = today + timedelta(days=1)
            return tomorrow.strftime("%Y-%m-%d")
        elif relative_date == "ì£¼ë§":
            days_until_saturday = (5 - today.weekday() + 7) % 7
            if days_until_saturday == 0:
                days_until_saturday = 7
            weekend_saturday = today + timedelta(days=days_until_saturday)
            return weekend_saturday.strftime("%Y-%m-%d")
        elif relative_date == "ì´ë²ˆì£¼":
            days_until_monday = today.weekday()
            this_monday = today - timedelta(days=days_until_monday)
            return this_monday.strftime("%Y-%m-%d")
        elif relative_date == "ë‹¤ìŒì£¼":
            days_until_monday = (0 - today.weekday() + 7) % 7
            next_monday = today + timedelta(days=days_until_monday)
            return next_monday.strftime("%Y-%m-%d")
        else:
            return relative_date

    def _parse_single_sentence(self, sentence: str) -> Dict[str, Any]:
        print(f"\n[STEP 1] ì›ë³¸ ë¬¸ì¥: '{sentence}'")

        # 1. ì‹ ì¡°ì–´ ì²˜ë¦¬ í›„ Mecab í’ˆì‚¬ íƒœê¹… 
        parsed_tokens = self.tokenizer.pos(sentence)
        for word in self.special_words:
            if word in sentence:
                processed_tokens = []
                current_text = sentence
                while word in current_text:
                    pre_word_text, post_word_text = current_text.split(word, 1)
                    
                    processed_tokens.extend(self.tokenizer.pos(pre_word_text))
                    processed_tokens.append((word, "NNG"))
                    current_text = post_word_text
                
                processed_tokens.extend(self.tokenizer.pos(current_text))
                
                parsed_tokens = [(t, p) for t, p in processed_tokens if t.strip()]
                break # ì²« ë²ˆì§¸ íŠ¹ìˆ˜ ë‹¨ì–´ë§Œ ì²˜ë¦¬

        print(f"[STEP 2] Mecab í’ˆì‚¬ íƒœê¹… ê²°ê³¼: {parsed_tokens}")

        date = ""
        time = ""
        metadata_tokens = set()
        final_verb_root = ""
        action_verbs = []
        object_nouns = set() # ğŸš¨ FIX: í•µì‹¬ ëª©ì ë¬¼(ì„/ë¥¼, ì´/ê°€) ì‹ë³„ìš©

        # 2. ë©”íƒ€ë°ì´í„°, ë™ì‚¬, í•µì‹¬ ëª©ì ì–´/ì£¼ì–´ ì¶”ì¶œ
        for i, (token, pos) in enumerate(parsed_tokens):
            
            # í•µì‹¬ ëª©ì ì–´/ì£¼ì–´ ì‹ë³„ (JKO, JKS)
            if pos.startswith("NN") or pos.startswith("NP"):
                if i + 1 < len(parsed_tokens) and parsed_tokens[i+1][1].startswith("JKO"): # ëª©ì ê²© (ì„/ë¥¼)
                    object_nouns.add(token)
                if i + 1 < len(parsed_tokens) and parsed_tokens[i+1][1].startswith("JKS"): # ì£¼ê²© (ì´/ê°€)
                    object_nouns.add(token)
            
            # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (ë‚ ì§œ/ì‹œê°„)
            if token in ["ë‚´ì¼", "ì˜¤ëŠ˜", "ì´ë²ˆì£¼", "ë‹¤ìŒì£¼", "ì£¼ë§"]:
                date = self._get_absolute_date(token)
                metadata_tokens.add((token, pos))
            elif token in ["ì•„ì¹¨", "ì ì‹¬", "ì €ë…", "ì˜¤ì „", "ì˜¤í›„", "ìƒˆë²½", "ë°¤", "ë‚®"]:
                time = token
                metadata_tokens.add((token, pos))
            elif pos == "SN":
                if i + 1 < len(parsed_tokens) and parsed_tokens[i + 1][0] == "ì‹œ":
                    time = f"{token}ì‹œ"
                    metadata_tokens.add((token, pos))
                    metadata_tokens.add(parsed_tokens[i + 1])
                elif "ì‹œ" in sentence:
                    time = f"{token}ì‹œ"
                    metadata_tokens.add((token, pos))
                else:
                    metadata_tokens.add((token, pos))
            elif token == "ì‹œ" and pos == "NNBC":
                metadata_tokens.add((token, pos))
                
            # ëª¨ë“  ì•¡ì…˜ ë™ì‚¬(VV, XSV) ìˆ˜ì§‘
            is_action_verb = pos.startswith("VV") or pos.startswith("XSV")
            
            if is_action_verb: 
                if not (token.startswith("ìˆ") or token.startswith("ì—†")):
                    action_verbs.append((token, pos))
                metadata_tokens.add((token, pos))
            elif pos == "VA": 
                metadata_tokens.add((token, pos))

            elif token == "ìì•¼" and pos == "NNG" and 'ì ' in sentence:
                action_verbs.append(("ì", "VV"))
                metadata_tokens.add((token, pos))
        
        # 2-1. ì¶”ì¶œëœ ë™ì‚¬ ëª©ë¡ì—ì„œ ìµœì¢… ë™ì‚¬ ê²°ì •: 
        # ğŸš¨ FIX 5: ë©”ì¸ ë™ì‚¬(ë³´ì¡° ë™ì‚¬ê°€ ì•„ë‹Œ)ë¥¼ ìš°ì„  ì„ íƒí•˜ì—¬ 'ì§‘ ì‰¬ê¸°' ë¬¸ì œ í•´ê²°
        for token, pos in reversed(action_verbs):
            if token == "ë˜": continue 
            
            root = self._get_verb_root(token)

            # 'ì‰¬ì–´ì•¼ í•´'ì²˜ëŸ¼ ë§ˆì§€ë§‰ì— 'í•˜' ê³„ì—´(ë³´ì¡° ë™ì‚¬)ì´ ì˜¤ë©´ ê±´ë„ˆë›°ê³  ì´ì „ ë™ì‚¬ ì°¾ê¸°
            # 'í•´'ê°€ ìœ ì¼í•œ ë™ì‚¬ì´ë©´ 'í•˜'ê°€ ì„ íƒë¨
            if root == "í•˜" and token in ["í•´", "í•´ì•¼", "í•©ë‹ˆë‹¤", "ë´ì•¼", "ë´"]: 
                if len(action_verbs) == 1:
                    final_verb_root = root
                    break
                else:
                    continue # ë‹¤ìŒ (ë©”ì¸) ë™ì‚¬ë¥¼ ì°¾ìŒ
            
            if root:
                final_verb_root = root
                break

        # 3. todo_partsì— ìœ íš¨í•œ ëª…ì‚¬ë§Œ ì¶”ì¶œ 
        todo_parts = []
        is_jam_in_sentence = 'ì ' in sentence
            
        for i, (token, pos) in enumerate(parsed_tokens): # ì¸ë±ìŠ¤ ì‚¬ìš©ì„ ìœ„í•´ enumerate ì¬ì‚¬ìš©
            
            # 3-1. í•„í„°ë§: ê¸°ëŠ¥ì–´, ë™ì‚¬, í˜•ìš©ì‚¬, ë©”íƒ€ë°ì´í„° í† í° ì œì™¸
            is_functional_or_verb = (
                pos.startswith("J") or pos.startswith("E") or pos.startswith("M") or 
                pos.startswith("X") or pos.startswith("S") or 
                pos.startswith("VV") or pos.startswith("VA") or 
                (token, pos) in metadata_tokens 
            )
            if is_functional_or_verb:
                continue
            
            # 3-2. ëª…ì‚¬ í•„í„°ë§ ê°•í™”: ëŒ€ëª…ì‚¬(NP) ë° Mecab ì˜¤ë¶„ë¥˜ í•„í„°ë§
            if pos == "NP" and token in ["ë‚˜", "ë„ˆ", "ìš°ë¦¬", "ì €"]: 
                continue

            if pos.startswith("NN") or pos.startswith("XSN") or pos.startswith("SL"): 
                
                # ğŸš¨ NEW FIX: JKB ëª…ì‚¬ í•„í„°ë§ (ë§¥ë½ ëª…ì‚¬ ì œê±°)
                is_followed_by_jkb = False
                if i + 1 < len(parsed_tokens) and parsed_tokens[i+1][1].startswith("JKB"):
                    is_followed_by_jkb = True

                # í•µì‹¬ ëª©ì ì–´/ì£¼ì–´ê°€ ì¡´ì¬í•˜ê³ (object_nouns), í˜„ì¬ ëª…ì‚¬ê°€ JKBì™€ í•¨ê»˜ ì“°ì¸ ê²½ìš°, ëª©ì ë¬¼ì´ ì•„ë‹ˆë©´ ì œê±°
                # ì´ ë¡œì§ìœ¼ë¡œ 'ì§‘ ê¸¸ ë‘ë¶€ ì‚¬ê¸°'ì—ì„œ 'ì§‘', 'ê¸¸'ì´ ì œê±°ë¨
                if object_nouns and is_followed_by_jkb and token not in object_nouns:
                    continue 
                
                # (ê¸°ì¡´: 'ì  ìê¸°' ì¼€ì´ìŠ¤ í•„í„°ë§ ìœ ì§€)
                if token == "ìì•¼" and pos == "NNG" and is_jam_in_sentence:
                    continue
                
                todo_parts.append(token)
            
        # 4. ìµœì¢… í•  ì¼ ë¬¸ì¥ ìƒì„±: ëª…ì‚¬êµ¬ + [ë™ì‚¬ì›í˜•]ê¸°
        
        # ğŸš¨ FIX 3: special_wordsì— ìˆëŠ” ë‹¨ì–´ëŠ” ë¶™ì—¬ì„œ ë‚˜ì˜¤ë„ë¡ ì²˜ë¦¬
        temp_noun_phrase = " ".join(todo_parts)
        for word in self.special_words:
            # Mecabì´ ìª¼ê°  ëª…ì‚¬ë¥¼ ë‹¤ì‹œ ë¶™ì¸ë‹¤ (ì˜ˆ: "ì  ë©”ì¶”" -> "ì ë©”ì¶”")
            if ' ' in word: 
                 continue
                 
            split_word = " ".join(self.tokenizer.morphs(word))
            temp_noun_phrase = temp_noun_phrase.replace(split_word, word)

        todo_parts = [p for p in temp_noun_phrase.split(" ") if p]
        todo_noun_phrase = " ".join(todo_parts).strip()
        final_todo = ""
        last_noun = todo_parts[-1] if todo_parts else ""

        if final_verb_root:
            verb_noun = final_verb_root + "ê¸°"
            
            # ğŸš¨ FIX 1: ë™ì‚¬ì„± ëª…ì‚¬ ('ì‘ì„±', 'ê²€ìƒ‰', 'ìš´ë™' ë“±) + 'í•˜ê¸°'ëŠ” ë¶™ì—¬ì“°ê¸°
            if final_verb_root == 'í•˜' and last_noun in ["ì‘ì„±", "ê²€ìƒ‰", "ìš´ë™", "ì¤€ë¹„", "ì •ë¦¬"]:
                rest_of_nouns = " ".join(todo_parts[:-1]).strip()
                
                if rest_of_nouns:
                    final_todo = rest_of_nouns + " " + last_noun + verb_noun
                else:
                    final_todo = last_noun + verb_noun
            
            # ê·¸ ì™¸ì˜ ì¼ë°˜ì ì¸ ëª…ì‚¬êµ¬ + ë™ì‚¬ ì¡°í•© ('ë‘ë¶€ ì‚¬ê¸°', 'ì§‘ ì‰¬ê¸°', 'ì  ìê¸°')
            elif todo_noun_phrase:
                final_todo = todo_noun_phrase + " " + verb_noun
            else:
                final_todo = verb_noun 
        
        # 5. Todoê°€ ë¹„ì–´ìˆìœ¼ë©´ ë¬¸ì¥ ì „ì²´ë¥¼ ë‹¤ì‹œ ì‚¬ìš© (ì˜ˆì™¸ ì²˜ë¦¬)
        if not final_todo.strip():
            final_todo = sentence

        print("\n--- íŒŒì‹± ê²°ê³¼ ìµœì¢… í™•ì¸ ---")
        print(f"Todo: '{final_todo}'")
        print(f"Date: '{date}'")
        print(f"Time: '{time}'")
        print("----------------------------\n")

        return {"todo": final_todo, "date": date, "time": time, "original_sentence": sentence}

    def parse_multiple_sentences(self, text: str) -> List[Dict[str, Any]]:
        print(f"ì „ì²´ ì…ë ¥ í…ìŠ¤íŠ¸: '{text}'")
        sentences = self._split_sentences(text) 

        parsed_results = []
        last_known_date = datetime.today().strftime("%Y-%m-%d")

        for sentence in sentences:
            if not sentence:
                continue

            result = self._parse_single_sentence(sentence)

            if result["todo"] in self.SPLIT_TEXTS or result["todo"] in ["ê³ ", "ê·¸ë¦¬ê³ "]:
                 print(f"âš ï¸ ë¶„ë¦¬ í† í° í•„í„°ë§: {result['todo']}")
                 continue

            NOISE_TOKENS = ["ë‚˜", "ê³ ", "ê·¸ë¦¬ê³ ", "ë˜", "ë¼"] 
            if not result['todo'].strip() or (len(result['todo'].split()) == 1 and result['todo'].strip() in NOISE_TOKENS):
                 print(f"âš ï¸ ë‹¨ì¼ í† í° í•„í„°ë§: {result['todo']}")
                 continue


            if result["date"]:
                last_known_date = result["date"]
            elif last_known_date:
                result["date"] = last_known_date

            parsed_results.append(result)

        return parsed_results


if __name__ == "__main__":
    parser_instance = Parser()

    # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 1: ë„ì–´ì“°ê¸° ë° ì˜¤ë¶„ë¥˜, ë§¥ë½ í•„í„°ë§ í…ŒìŠ¤íŠ¸
    # ê¸°ëŒ€: í¬íŠ¸í´ë¦¬ì˜¤ ì‘ì„±í•˜ê¸°, ì  ìê¸°, ì±„ìš©ê³µê³  ê²€ìƒ‰í•˜ê¸°, ë‘ë¶€ ì‚¬ê¸° (<- ë³€ê²½ë¨)
    input_text_1 = "ë‚˜ëŠ” í¬íŠ¸í´ë¦¬ì˜¤ ì‘ì„± í•´ì•¼ í•˜ê³  ë°¤ì—” ì ì„ ì˜ ìì•¼ë˜ê³  ì±„ìš©ê³µê³ ë¥¼ ê²€ìƒ‰í•´ ë´ì•¼ í•©ë‹ˆë‹¤ ê·¸ë¦¬ê³  ì§‘ì— ê°€ëŠ” ê¸¸ì— ë‘ë¶€ë¥¼ ì‚¬ì•¼ ë¼"
    print("\n--- í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 1 ì‹¤í–‰ (ë„ì–´ì“°ê¸° ë° í•„í„°ë§) ---\n")
    parsed_list_1 = parser_instance.parse_multiple_sentences(input_text_1)

    print("\n\n--- ìµœì¢… JSON ì¶œë ¥ (í…ŒìŠ¤íŠ¸ 1) ---")
    print(json.dumps(parsed_list_1, indent=4, ensure_ascii=False))

    # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 2: ê¸°ì¡´ í…ŒìŠ¤íŠ¸ ìœ ì§€ (ì •ìƒ ì‘ë™ í™•ì¸)
    # ê¸°ëŒ€: í—¬ìŠ¤ì¥ ê°€ì„œ ìš´ë™í•˜ê¸°, ì ë©”ì¶” ì—½ë–¡ ë¨¹ê¸°
    input_text_2 = "ë‚´ì¼ ì•„ì¹¨ 9ì‹œì— í—¬ìŠ¤ì¥ ê°€ì„œ ìš´ë™ í•˜ê³  ê·¸ë¦¬ê³  ì ë©”ì¶” ë°›ì•„ì„œ ì—½ë–¡ ë¨¹ì–´ì•¼ì§€"
    print("\n--- í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 2 ì‹¤í–‰ (ì‹œê°„/ì¥ì†Œ ë©”íƒ€ë°ì´í„°) ---\n")
    parsed_list_2 = parser_instance.parse_multiple_sentences(input_text_2)

    print("\n\n--- ìµœì¢… JSON ì¶œë ¥ (í…ŒìŠ¤íŠ¸ 2) ---")
    print(json.dumps(parsed_list_2, indent=4, ensure_ascii=False))

    # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 3: ì‚¬ìš©ì ìŒì„± ì˜ˆì‹œ
    # ê¸°ëŒ€: ë‘ë¶€ ì‚¬ê¸°, ê²½ì°°ì„œ ê°€ê¸°, ì§‘ ì‰¬ê¸°
    input_text_3 = "ì˜¤ëŠ˜ ì¼ë‹¨ ë‘ë¶€ ì‚¬ì•¼ í•˜ê³  ê²½ì°°ì„œ ê°€ì•¼ í•˜ê³  ì§‘ì—ì„œ ì¢€ ì˜ ì‰¬ì–´ì•¼ í•´"
    print("\n--- í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 3 ì‹¤í–‰ (ê°„ê²°í•œ êµ¬ì–´ì²´) ---\n")
    parsed_list_3 = parser_instance.parse_multiple_sentences(input_text_3)

    print("\n\n--- ìµœì¢… JSON ì¶œë ¥ (í…ŒìŠ¤íŠ¸ 3) ---")
    print(json.dumps(parsed_list_3, indent=4, ensure_ascii=False))