import re
import json
from typing import Dict, Any, List
from mecab import MeCab
from datetime import datetime, timedelta
import os

class Parser:
    def __init__(self):
        self.tokenizer = MeCab()
        print("Parser ì´ˆê¸°í™” ì™„ë£Œ: Mecab ì—”ì§„ ì‚¬ìš©")

        self.special_words = [
            "ì—½ë–¡", "ì§œíŒŒêµ¬ë¦¬", "ë§ë‹´", "ì¸ê°•", "ì¿ íŒ¡", "ë°°ë¯¼", "ìš”ê¸°ìš”", "ë¡œì œ", "í˜¼ìˆ ",
            "í˜¼ë°¥", "ì†Œí™•í–‰", "í‡´ê·¼ê¸¸", "ì¶œê·¼ê¸¸", "ì ë©”ì¶”", "ì•„ì•„", "ì•„ë©”", "ì•„ì¹´",
            "ì•„ì¹´í˜ë¼", "ì¹´í˜ë¼ë–¼", "ì¹´í˜ëª¨ì¹´", "ì¹´ëª¨", "ì¹´ëª¨ì¹´", "í—¬ìŠ¤ì¥", "êµì´Œì¹˜í‚¨", "í¬íŠ¸í´ë¦¬ì˜¤"
        ]

        # ë¬¸ì¥ ë¶„ë¦¬ ê¸°ì¤€ í’ˆì‚¬/í† í° ëª©ë¡
        self.SPLIT_TOKENS = {
            "EC",  # ì—°ê²° ì–´ë¯¸: -ê³ , -ìœ¼ë©° ë“±
            "MAJ",  # ì ‘ì† ë¶€ì‚¬: ê·¸ë¦¬ê³ , ê·¸ëŸ¬ë‚˜ ë“±
            "SF"   # ë§ˆì¹¨í‘œ
        }
        # í† í° í…ìŠ¤íŠ¸ ê¸°ì¤€ (ê°•ì œ ë¶„ë¦¬ ë° í•„í„°ë§)
        self.SPLIT_TEXTS = [
            "ê·¸ë¦¬ê³ ", "ê·¸ëŸ¬ê³ ", "í•´ì•¼ì§€", "í•´ì•¼ê² ë‹¤", "í•´ì•¼ë¼", "í•´ì•¼ë§Œ", "í•˜ê³ ", "ì´ê³ ", "ë˜", "ë˜ë‹ˆ", "ë˜ì„œ", "ë˜ê³ ", "ê³ ", "ë¼"
        ]

    def _split_sentences(self, text: str) -> List[str]:
        tokens = self.tokenizer.pos(text)
        
        sentences = []
        current_sentence_tokens = []

        for i, (token, pos) in enumerate(tokens):
            current_sentence_tokens.append(token)
            
            is_split_point = pos in self.SPLIT_TOKENS or token in self.SPLIT_TEXTS

            if is_split_point:
                sentence = "".join(current_sentence_tokens).strip()
                if sentence:
                    sentences.append(sentence)
                current_sentence_tokens = []

            if i == len(tokens) - 1 and current_sentence_tokens:
                 sentence = "".join(current_sentence_tokens).strip()
                 if sentence:
                     sentences.append(sentence)

        return [s for s in sentences if s.strip()]

    def _get_absolute_date(self, relative_date: str) -> str:
        """ ìƒëŒ€ì  ë‚ ì§œë¥¼ ì ˆëŒ€ ë‚ ì§œë¡œ ë³€í™˜ (ê¸°ì¡´ ë¡œì§ ìœ ì§€) """
        today = datetime.today()
        if relative_date == "ì˜¤ëŠ˜" or relative_date == "":
            return today.strftime("%Y-%m-%d")
        elif relative_date == "ë‚´ì¼":
            tomorrow = today + timedelta(days=1)
            return tomorrow.strftime("%Y-%m-%d")
        elif relative_date == "ì£¼ë§":
            days_until_saturday = (5 - today.weekday() + 7) % 7
            if days_until_saturday == 0:
                days_until_saturday = 7
            weekend_saturday = today + timedelta(days=days_until_saturday)
            return weekend_saturday.strftime("%Y-%m-%d")
        elif relative_date == "ì´ë²ˆì£¼":
            days_until_monday = today.weekday()
            this_monday = today - timedelta(days=days_until_monday)
            return this_monday.strftime("%Y-%m-%d")
        elif relative_date == "ë‹¤ìŒì£¼":
            days_until_monday = (0 - today.weekday() + 7) % 7
            next_monday = today + timedelta(days=days_until_monday)
            return next_monday.strftime("%Y-%m-%d")
        else:
            return relative_date

    def _parse_single_sentence(self, sentence: str) -> Dict[str, Any]:
        print(f"\n[STEP 1] ì›ë³¸ ë¬¸ì¥: '{sentence}'")

        # 1. ì‹ ì¡°ì–´ ì²˜ë¦¬ í›„ Mecab í’ˆì‚¬ íƒœê¹… (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
        parsed_tokens = self.tokenizer.pos(sentence)
        for word in self.special_words:
            if word in sentence:
                processed_tokens = []
                current_text = sentence
                while word in current_text:
                    pre_word_text, post_word_text = current_text.split(word, 1)
                    processed_tokens.extend(self.tokenizer.pos(pre_word_text.strip()))
                    processed_tokens.append((word, "NNG"))
                    current_text = post_word_text.strip()
                processed_tokens.extend(self.tokenizer.pos(current_text))
                parsed_tokens = processed_tokens
                break
        
        print(f"[STEP 2] Mecab í’ˆì‚¬ íƒœê¹… ê²°ê³¼: {parsed_tokens}")

        date = ""
        time = ""
        metadata_tokens = set()
        final_verb_root = ""
        action_verbs = []

        # 2. ë©”íƒ€ë°ì´í„° ë° ëª¨ë“  ì•¡ì…˜ ë™ì‚¬ ì¶”ì¶œ
        for i, (token, pos) in enumerate(parsed_tokens):
            # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (ë‚ ì§œ/ì‹œê°„)
            if token in ["ë‚´ì¼", "ì˜¤ëŠ˜", "ì´ë²ˆì£¼", "ë‹¤ìŒì£¼", "ì£¼ë§"]:
                date = self._get_absolute_date(token)
                metadata_tokens.add((token, pos))
            elif token in ["ì•„ì¹¨", "ì ì‹¬", "ì €ë…", "ì˜¤ì „", "ì˜¤í›„", "ìƒˆë²½", "ë°¤", "ë‚®"]:
                time = token
                metadata_tokens.add((token, pos))
            elif pos == "SN":
                if i + 1 < len(parsed_tokens) and parsed_tokens[i + 1][0] == "ì‹œ":
                    time = f"{token}ì‹œ"
                    metadata_tokens.add((token, pos))
                    metadata_tokens.add(parsed_tokens[i + 1])
                elif "ì‹œ" in sentence:
                    time = f"{token}ì‹œ"
                    metadata_tokens.add((token, pos))
                else:
                    metadata_tokens.add((token, pos))
            elif token == "ì‹œ" and pos == "NNBC":
                metadata_tokens.add((token, pos))
                
            # ëª¨ë“  ì•¡ì…˜ ë™ì‚¬(VV, XSV) ìˆ˜ì§‘
            elif pos in ["VV", "XSV"]: 
                if not (token.startswith("ìˆ") or token.startswith("ì—†")):
                    action_verbs.append((token, pos))
                metadata_tokens.add((token, pos))
            elif pos == "VA": 
                metadata_tokens.add((token, pos))

            # 'ì  ìê¸°' ì¼€ì´ìŠ¤ë¥¼ ìœ„í•œ ì˜¤ë¶„ë¥˜ 'ìì•¼' ì²˜ë¦¬
            elif token == "ìì•¼" and pos == "NNG" and 'ì ' in sentence:
                action_verbs.append(("ì", "VV"))
                metadata_tokens.add((token, pos))
        
        # 2-1. ì¶”ì¶œëœ ë™ì‚¬ ëª©ë¡ì—ì„œ ìµœì¢… ë™ì‚¬ ê²°ì •: 'ë˜' í•„í„°ë§ í›„ ê°€ì¥ ë§ˆì§€ë§‰ ë™ì‚¬ ì„ íƒ
        for token, pos in reversed(action_verbs):
            if token != "ë˜": 
                final_verb_root = token
                break

        # 3. todo_partsì— ìœ íš¨í•œ ëª…ì‚¬ë§Œ ì¶”ì¶œ ë° ë¶ˆí•„ìš”í•œ ëª…ì‚¬/ëŒ€ëª…ì‚¬ í•„í„°ë§
        todo_parts = []
        is_jam_in_sentence = 'ì ' in sentence
        
        # ğŸš¨ NEW LOGIC: JKB(ë¶€ì‚¬ê²© ì¡°ì‚¬)ê°€ ë’¤ë”°ë¥´ëŠ” ëª…ì‚¬ë¥¼ ë§¥ë½ìœ¼ë¡œ ê°„ì£¼í•˜ì—¬ í•„í„°ë§
        nouns_to_filter = set()
        for i, (token, pos) in enumerate(parsed_tokens):
            if pos.startswith("NN") or pos.startswith("NP"):
                # Noun/NPê°€ JKB(ë¶€ì‚¬ê²© ì¡°ì‚¬)ë¡œ ê³§ë°”ë¡œ ì´ì–´ì§€ë©´ í•„í„°ë§
                if i + 1 < len(parsed_tokens) and parsed_tokens[i+1][1].startswith("JKB"):
                     nouns_to_filter.add((token, pos))
        
        # í•„í„°ë§ëœ ëª…ì‚¬ë¥¼ metadata_tokensì— ì¶”ê°€í•˜ì—¬ ëª…ì‚¬ ì¶”ì¶œ ì‹œ ì œì™¸ë˜ë„ë¡ í•¨
        for noun_item in nouns_to_filter:
            metadata_tokens.add(noun_item)
            
        for token, pos in parsed_tokens:
            
            # 3-1. í•„í„°ë§: ê¸°ëŠ¥ì–´, ë™ì‚¬, í˜•ìš©ì‚¬, ë©”íƒ€ë°ì´í„° í† í°(ì‹œê°„, ë‚ ì§œ, JKB í•„í„°ë§ ëª…ì‚¬) ì œì™¸
            is_functional_or_verb = (
                pos.startswith("J") or pos.startswith("E") or pos.startswith("M") or 
                pos.startswith("X") or pos.startswith("S") or 
                pos in ["VV", "VA", "XSV"] or 
                (token, pos) in metadata_tokens 
            )
            if is_functional_or_verb:
                continue
            
            # 3-2. ëª…ì‚¬ í•„í„°ë§ ê°•í™”: ëŒ€ëª…ì‚¬(NP) ë° Mecab ì˜¤ë¶„ë¥˜ í•„í„°ë§
            if pos == "NP" and token in ["ë‚˜", "ë„ˆ", "ìš°ë¦¬", "ì €"]: 
                continue

            if pos.startswith("NN") or pos.startswith("XSN") or pos.startswith("SL"): 
                # 'ì  ìê¸°' ì¼€ì´ìŠ¤ë¥¼ ìœ„í•œ ì˜¤ë¶„ë¥˜ 'ìì•¼' í•„í„°ë§
                if token == "ìì•¼" and pos == "NNG" and is_jam_in_sentence:
                    continue
                
                todo_parts.append(token)
            
        # 4. ìµœì¢… í•  ì¼ ë¬¸ì¥ ìƒì„±: ëª…ì‚¬êµ¬ + [ë™ì‚¬ì›í˜•]ê¸°
        
        todo_noun_phrase = " ".join(todo_parts).strip()
        final_todo = todo_noun_phrase

        if final_verb_root:
            verb_noun = final_verb_root + "ê¸°"
            
            if final_todo:
                final_todo += " " + verb_noun
            else:
                final_todo = verb_noun 
        
        # 5. Todoê°€ ë¹„ì–´ìˆìœ¼ë©´ ë¬¸ì¥ ì „ì²´ë¥¼ ë‹¤ì‹œ ì‚¬ìš© (ì˜ˆì™¸ ì²˜ë¦¬)
        if not final_todo.strip():
            final_todo = sentence

        print("\n--- íŒŒì‹± ê²°ê³¼ ìµœì¢… í™•ì¸ ---")
        print(f"Todo: '{final_todo}'")
        print(f"Date: '{date}'")
        print(f"Time: '{time}'")
        print("----------------------------\n")

        return {"todo": final_todo, "date": date, "time": time, "original_sentence": sentence}

    def parse_multiple_sentences(self, text: str) -> List[Dict[str, Any]]:
        print(f"ì „ì²´ ì…ë ¥ í…ìŠ¤íŠ¸: '{text}'")
        sentences = self._split_sentences(text)

        parsed_results = []
        last_known_date = datetime.today().strftime("%Y-%m-%d")

        for sentence in sentences:
            if not sentence:
                continue

            result = self._parse_single_sentence(sentence)

            # 1. Todoê°€ ë‹¨ì¼ ë¶„ë¦¬ í† í°ì¸ ê²½ìš° (e.g., "ê³ ", "ê·¸ë¦¬ê³ ") í•„í„°ë§
            if result["todo"] in self.SPLIT_TEXTS or result["todo"] in ["ê³ ", "ê·¸ë¦¬ê³ "]:
                 print(f"âš ï¸ ë¶„ë¦¬ í† í° í•„í„°ë§: {result['todo']}")
                 continue

            # 2. ê²°ê³¼ê°€ ë‹¨ë… ëª…ì‚¬/ë™ì‚¬/ì¡í† í°ì´ê³  í•  ì¼ì´ ì•„ë‹Œ ê²½ìš° í•„í„°ë§ (í•„í„°ë§ ëª©ë¡ì— 'ë¼' ì¶”ê°€)
            NOISE_TOKENS = ["ë‚˜", "ê³ ", "ê·¸ë¦¬ê³ ", "ë˜", "ë¼"] 
            if not result['todo'].strip() or (len(result['todo'].split()) == 1 and result['todo'].strip() in NOISE_TOKENS):
                 print(f"âš ï¸ ë‹¨ì¼ í† í° í•„í„°ë§: {result['todo']}")
                 continue


            if result["date"]:
                last_known_date = result["date"]
            elif last_known_date:
                result["date"] = last_known_date

            parsed_results.append(result)

        return parsed_results


if __name__ == "__main__":
    parser_instance = Parser()

    # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 1: ìƒˆë¡œìš´ ì…ë ¥ í…ŒìŠ¤íŠ¸
    input_text_1 = "ë‚˜ëŠ” í¬íŠ¸í´ë¦¬ì˜¤ ì‘ì„± í•´ì•¼ í•˜ê³  ë°¤ì—” ì ì„ ì˜ ìì•¼ë˜ê³  ì±„ìš©ê³µê³ ë¥¼ ê²€ìƒ‰í•´ ë´ì•¼ í•©ë‹ˆë‹¤ ê·¸ë¦¬ê³  ì§‘ì— ê°€ëŠ” ê¸¸ì— ë‘ë¶€ë¥¼ ì‚¬ì•¼ ë¼"
    print("\n--- í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 1 ì‹¤í–‰ ---\n")
    parsed_list_1 = parser_instance.parse_multiple_sentences(input_text_1)

    print("\n\n--- ìµœì¢… JSON ì¶œë ¥ (í…ŒìŠ¤íŠ¸ 1) ---")
    print(json.dumps(parsed_list_1, indent=4, ensure_ascii=False))

    # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 2: ê¸°ì¡´ í…ŒìŠ¤íŠ¸ ìœ ì§€ (ì •ìƒ ì‘ë™ í™•ì¸)
    input_text_2 = "ë‚´ì¼ ì•„ì¹¨ 9ì‹œì— í—¬ìŠ¤ì¥ ê°€ì„œ ìš´ë™ í•˜ê³  ê·¸ë¦¬ê³  ì ë©”ì¶” ë°›ì•„ì„œ ì—½ë–¡ ë¨¹ì–´ì•¼ì§€"
    print("\n--- í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 2 ì‹¤í–‰ ---\n")
    parsed_list_2 = parser_instance.parse_multiple_sentences(input_text_2)

    print("\n\n--- ìµœì¢… JSON ì¶œë ¥ (í…ŒìŠ¤íŠ¸ 2) ---")
    print(json.dumps(parsed_list_2, indent=4, ensure_ascii=False))